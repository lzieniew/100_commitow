High level overwiev of what has to be done for YarnYap - article to speech prgram, that will convert article or reddit thread into youtube video
  0. [x] Create a basic docker configuration with docker-compose, so the software can be run easily
  1. [x] Create Backend component and database, create simple endpoint for adding url to database
  2. [x] Create a background worker, make the basic mechanism of fetching the urls from database
  3. [x] Implement changing of the statuses in the worker
  4. [x] Define cleaning mechanism requirements, with basic manual tests using the styletts engine
  5. [x] Use coqui TTS as voice generation library
  6  [x] Get coqui to run in the dummy endpoint
  7. [x] Design and create an API that returns sound as a response
  8. [x] Implement receiving the raw sound and saving it as sound file in the worker
  9. [x] Create a service that will actually generate voice from text
  10. [x] Use the voice generating service in background worker, also implement mechanism for splitting the text into chunks which can be processed at once
  11. [x] implement handling inactivity of the text generator container
  12. [x] implement recognising the whole text language as fallback
  13. [x] add some alternative generation engine with lower requirements for development purposes, but also for less powerful machines
  14. [x] Implement caching mechanism, and by doing this also add some info to Job and maybe child model, like the method of generation (CPU vs GPU), generation time for every chunk, actual data etc. (to be determined if the generation data should go inside Job model, as children of Job, or to different database)
  15. [ ] Implement cleaning mechanism in the background worker - urls, long sentences splitting etc
  16. [ ] Write tests for cleaning
  17. [ ] Check if there is some way to clean already generated audio from those strange glitches
  18. [ ] Create the fetching mechanism
  19. [ ] Create a frontend in VUE js
    19.0 [ ] refresh html + css + js xD
    19.1 [ ] learn VUE xD
  20. [ ] Adapt fetching mechanism for Reddit
  21. [ ] To be decided - maybe use whisper to verify the output text, even only to tell user to check if those clips are okay


Test sanitization and generation on this article: https://daniel.haxx.se/blog/2024/01/02/the-i-in-llm-stands-for-intelligence/)


Other things todo:
  - [x] make the list jobs endpoint return ids, that are visible in swagger
  - [x] fix import from shared in worker
  - [x] optimize Dockerfiles, so change in shared_components won't trigger reinstallation of all requirements
  - [x] setup tests with mongodb
  - [x] implement saving elapsed time in Job model, and also somehow printing it in every call
  - [x] Fix issue with now generating text "Ale jajo!" xD It's detected as 'sl', so maybe it's better to detect text language as a whole - but then single sentences in other languages will not work
  - [x] make initialization only once at text_to_speech service startup, instead every time the endpoint is called, and measure difference
  - [x] Sanitazed text should be kept in mongo as table of sentences
  - [x] Add progress field for generating - it will be useful for long texts
  - [x] Fix error and make the process resistant to errors
  - [x] fix error with accpeting the license
  - [x] fix the format returned by edge_tts
  - [x] change the sanitizing static website at port 8080 to remove also " and other problematic characters
  - [x] fix potential race with file saving in text to speech - add randomized file name and removing them after some time
  - [x] make sure that the model is not redownloading every ??? (I don't know when redownloading occurs, find this out!)
  - [x] check why player doesn't appear on text_to_speech api and appears on backend api
  - [x] fix utf-8 error 
  - [x] change Job.audio_path to Job.audio_content - so there will be no temporary files in repo. In time the content will be moved into subjobs, so even in the case of audiobook there will be no long files that would be greater than mongo limit
  - [x] Try to implement some resuming mechanism (very useful for long jobs), maybe other collection in mongo with saving small files - it can even be deleted after job is done. The key might be job_id + index of sentence - almost the same as 14. point above
  - [x] Keep the sentences order!
  - [x] return data about generation from the generation endpoint - like time, method (CPU/GPU) etc - in preparation of caching feature which will at the same time save more info about the generation
  - [x] Fix max size error when sum of all sentences audio reaches 16MB. I have 3 ideas:
  - [x] Check and try to split long sentences into shorter ones (there are warnings that pl has 224 chars limit, and en has 250 characters limit) - done in naive way, just by word_wrap - maybe will improve it
  - [x] Write integration tests - each one would have different text or url (in case of url the function fetching raw url will be mocked, to eliminate network calls in test) and I will check if the text was generated properly. They could also have "real mode", that wouldn not run on github pipeline and would actually generate audio, so I will be able to listen to it after some bigger changes and check if it changed
  - [ ] Expand integration tests: make active waiting, instead of sleep(20), check if audio file is correct, delete created jobs at the end.
  - [ ] implement uploading own voices
  - [ ] Change statuses to be more descriptive
  - [ ] Improve sentence splitting, especially on text with a lot of extra dots (np. Pasta o sp≈Çuczce do kibla). It's important because tts generates random sounds when encountering multiple dots
  - [ ] Fix error with duplicating jobs - maybe make different function for saving and upating
  - [ ] Fix progress in percent - right now it ignores skipped sentences, and doesn't reach 100%
  - [ ] Write worker tests, create mock text_to_speech api for it
  - [ ] ??? Implement automaticaly extracting sample voice from youtube link
  - [ ] Maybe implement some more sophisticated algorith to further split long sentences, instead wordwrap. It would probably have 3 stages: split by semicolons ; if sentences are still too long by comma , if still to long use standard word_wrap

Testing different TTS engines:
  - StyleTTS - current first choice, very fast, reasonable speed even on CPU only, good quality
  - https://github.com/jasonppy/VoiceCraft - to check, hard to run but very good quality
  - coqui TTS - license is worse than for example StyleTTS, but it soudns good in polish, so maybe I will try and use it
  - https://github.com/metavoiceio/metavoice-src - can try it, east to run from docker
  - https://github.com/netease-youdao/EmotiVoice - seems good, but only english and chinesee
  - RVC (based on VITS) or other speech to speech converter, it probably can improve quality
