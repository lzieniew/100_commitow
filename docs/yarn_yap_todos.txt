High level overwiev of what has to be done for YarnYap - article to speech prgram, that will convert article or reddit thread into youtube video
  0. [x] Create a basic docker configuration with docker-compose, so the software can be run easily
  1. [x] Create Backend component and database, create simple endpoint for adding url to database
  2. [x] Create a background worker, make the basic mechanism of fetching the urls from database
  3. [x] Implement changing of the statuses in the worker
  4. [x] Define cleaning mechanism requirements, with basic manual tests using the styletts engine
  5. [x] Use coqui TTS as voice generation library
  6  [x] Get coqui to run in the dummy endpoint
  7. [x] Design and create an API that returns sound as a response
  8. [x] Implement receiving the raw sound and saving it as sound file in the worker
  9. [x] Create a service that will actually generate voice from text
  10. [x] Use the voice generating service in background worker, also implement mechanism for splitting the text into chunks which can be processed at once
  11. [x] implement handling inactivity of the text generator container
  12. [x] implement recognising the whole text language as fallback
  13. [x] add some alternative generation engine with lower requirements for development purposes, but also for less powerful machines
  14. [ ] Implement caching mechanism, and by doing this also add some info to Job and maybe child model, like the method of generation (CPU vs GPU), generation time for every chunk, actual data etc. (to be determined if the generation data should go inside Job model, as children of Job, or to different database)
  15. [ ] Implement cleaning mechanism in the background worker
  16. [ ] Write tests for cleaning
  17. [ ] Check if there is some way to clean already generated audio from those strange glitches
  18. [ ] Create the fetching mechanism
  19. [ ] Create a frontend in VUE js
  20. [ ] Adapt fetching mechanism for Reddit


Test sanitization and generation on this article: https://daniel.haxx.se/blog/2024/01/02/the-i-in-llm-stands-for-intelligence/)


Other things todo:
  - [x] make the list jobs endpoint return ids, that are visible in swagger
  - [x] fix import from shared in worker
  - [x] optimize Dockerfiles, so change in shared_components won't trigger reinstallation of all requirements
  - [x] setup tests with mongodb
  - [x] implement saving elapsed time in Job model, and also somehow printing it in every call
  - [x] Fix issue with now generating text "Ale jajo!" xD It's detected as 'sl', so maybe it's better to detect text language as a whole - but then single sentences in other languages will not work
  - [x] make initialization only once at text_to_speech service startup, instead every time the endpoint is called, and measure difference
  - [x] Sanitazed text should be kept in mongo as table of sentences
  - [x] Add progress field for generating - it will be useful for long texts
  - [x] Fix error and make the process resistant to errors
  - [x] fix error with accpeting the license
  - [x] fix the format returned by edge_tts
  - [x] change the sanitizing static website at port 8080 to remove also " and other problematic characters
  - [ ] fix potential race with file saving in text to speech - add randomized file name and removing them after some time
  - [ ] implement uploading own voices
  - [ ] Change statuses to be more descriptive
  - [ ] Try to implement some resuming mechanism (very useful for long jobs), maybe other collection in mongo with saving small files - it can even be deleted after job is done. The key might be job_id + index of sentence - almost the same as 14. point above
  - [ ] Check and try to split long sentences into shorter ones (there are warnings that pl has 224 chars limit)
  - [ ] Improve sentence splitting, especially on text with a lot of extra dots (np. Pasta o sp≈Çuczce do kibla). It's important because tts generates random sounds when encountering multiple dots
  - [ ] Fix error with duplicating jobs - maybe make different function for saving and upating
  - [ ] Fix progress in percent - right now it ignores skipped sentences, and doesn't reach 100%
  - [ ] Write worker tests, create mock text_to_speech api for it
  - [ ] Write integration tests - each one would have different text or url (in case of url the function fetching raw url will be mocked, to eliminate network calls in test) and I will check if the text was generated properly. They could also have "real mode", that wouldn not run on github pipeline and would actually generate audio, so I will be abel to listen to it after some bigger changes and check if it changed
  - [ ] ??? Implement automaticaly extracting sample voice from youtube link

Testing different TTS engines:
  - StyleTTS - current first choice, very fast, reasonable speed even on CPU only, good quality
  - https://github.com/jasonppy/VoiceCraft - to check, hard to run but very good quality
  - coqui TTS - license is worse than for example StyleTTS, but it soudns good in polish, so maybe I will try and use it
  - https://github.com/metavoiceio/metavoice-src - can try it, east to run from docker
  - https://github.com/netease-youdao/EmotiVoice - seems good, but only english and chinesee
  - RVC (based on VITS) or other speech to speech converter, it probably can improve quality
